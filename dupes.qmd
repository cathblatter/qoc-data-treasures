# Duplicates

```{r, echo = F}
# load tibble, dplyr, naniar
library(readr)
library(tibble)
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(naniar))


zz_dataset <- read_csv("data/2023-02-20_QoC_sample_dataset.csv", 
                       show_col_types = FALSE)

dataset <- 
  zz_dataset %>% mutate(sex = as.factor(sex))
```

## Different 'types' of duplicates

- fully duplicated entries are 'easy' to catch but rare
- partially duplicated or mismatched entries are frequent but require more 
  sophisticated checking

### Fully duplicated entries

- `base::duplicated()` tests for fully duplicated entries (same row twice)

- several options to use: 

```{r}
# check numbers of duplicated rows
table(duplicated(dataset))
```

- add a column `is_dupe` with information:

```{r}
# adding column
dataset$is_dupe <- duplicated(dataset)

# check entry
head(dataset)
```


### Fully distinct entries

- `dplyr::distinct()` tests for full distinct entries (same row twice)

- basically the opposite of `base::duplicated()`


### Identify duplicates by counting the key (or key-index-pair)

```{r}
# counting the number of rows per patient_ids
# using sort = T moves the highest number on top
count(dataset, patient_ids, sort = T)
```

- there are several duplicated rows - although `duplicated()` only identified one 
fully duplicated entry

- this requires a further examination of these cases by identfiying the 
 "weird" ids and in a second create a subset (filtered) dataset, to check 
 these cases: 

```{r}
# keep the weird ids
weird_ids <- 
  count(dataset, patient_ids, sort = T) %>% 
  filter(n > 1 ) %>% 
  pull(patient_ids)

# filter the dataset by these values
dataset %>% 
  filter(patient_ids %in% weird_ids) %>% 
  arrange(patient_ids)
```







